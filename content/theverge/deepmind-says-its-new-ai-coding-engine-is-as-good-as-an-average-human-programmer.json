{
  "title": "DeepMind says its new AI coding engine is as good as an average human programmer",
  "url": "https://www.theverge.com/2022/2/2/22914085/alphacode-ai-coding-program-automatic-deepmind-codeforce",
  "date": "2022-02-02T16:00:00.000Z",
  "author": "James Vincent",
  "content": "      <figure>      <img alt=\"\" src=\"https://cdn.vox-cdn.com/thumbor/7z2oJvyYTeRG2Vt9ZFapFzMHuUs=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70462353/acastro_181017_1777_brain_ai_0002.0.jpg\" />        <figcaption>Illustration by Alex Castro / The Verge</figcaption>    </figure>  <p id=\"TGc2yj\">DeepMind has created an <a href=\"https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode\">AI system named AlphaCode</a> that it says “writes computer programs at a competitive level.” The Alphabet subsidiary tested its system against coding challenges used in human competitions and found that its program achieved an “estimated rank” placing it within the top 54 percent of human coders. The result is a significant step forward for autonomous coding, says DeepMind, though AlphaCode’s skills are not necessarily representative of the sort of programming tasks faced by the average coder. </p><p id=\"RcADoQ\">Oriol Vinyals, principal research scientist at DeepMind, told <em>The Verge</em> over email that the research was still in the early stages but that the results brought the company closer to creating a flexible problem-solving AI — a program that can autonomously tackle coding challenges that are currently the domain of humans only. “In the longer-term, we’re excited by [AlphaCode’s] potential for helping programmers and non-programmers write code, improving productivity or creating new ways of making software,” said Vinyals. </p><div class=\"c-float-right\"><aside id=\"jtrhZz\"><q>AlphaCode could be used to create coding assistants, and one day write its own software</q></aside></div><p id=\"nJlG4m\">AlphaCode was tested against challenges curated by <a href=\"https://codeforces.com/\">Codeforces</a>, a competitive coding platform that shares weekly problems and issues rankings for coders similar to the Elo rating system used in chess. These challenges are different from the sort of tasks a coder might face while making, say, a commercial app. They’re more self-contained and require a wider knowledge of both algorithms and theoretical concepts in computer science. Think of them as very specialized puzzles that combine logic, maths, and coding expertise. </p><p id=\"rtVQdC\">In one <a href=\"https://codeforces.com/problemset/problem/1553/D\">example challenge</a> that AlphaCode was tested on, competitors are asked to find a way to convert one string of random, repeated <em>s</em> and <em>t</em> letters into another string of the same letters using a limited set of inputs. Competitors cannot, for example, just type new letters but instead have to use a “backspace” command that deletes several letters in the original string. You can read a full description of the challenge below: </p>  <figure class=\"e-image\">        <img alt=\" \" data-mask-text=\"false\" src=\"https://cdn.vox-cdn.com/thumbor/3OWEV8HQw6l64DoGCRFUkQKSOcc=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/23212584/Screenshot_2022_02_02_at_14.30.00.png\">      <cite>Image: DeepMind / Codeforces</cite>      <figcaption><em>An example challenge titled “Backspace” that was used to evaluate DeepMind’s program. The problem is of medium difficulty, with the left side showing the problem description, and the right side showing example test cases. </em></figcaption>  </figure><p id=\"L4P0EQ\">Ten of these challenges were fed into AlphaCode in exactly the same format they’re given to humans. AlphaCode then generated a larger number of possible answers and winnowed these down by running the code and checking the output just as a human competitor might. “The whole process is automatic, without human selection of the best samples,” Yujia Li and David Choi, co-leads of the AlphaCode paper, told <em>The Verge</em> over email.</p><p id=\"0Fpns1\">AlphaCode was tested on 10 of challenges that had been tackled by 5,000 users on the Codeforces site. On average, it ranked within the top 54.3 percent of responses, and DeepMind estimates that this gives the system a Codeforces Elo of 1238, which places it within the top 28 percent of users who have competed on the site in the last six months.</p><p id=\"Va1Hw4\">“I can safely say the results of AlphaCode exceeded my expectations,” Codeforces founder Mike Mirzayanov said in a statement shared by DeepMind. “I was sceptical [sic] because even in simple competitive problems it is often required not only to implement the algorithm, but also (and this is the most difficult part) to invent it. AlphaCode managed to perform at the level of a promising new competitor.” </p>  <figure class=\"e-image\">        <img alt=\" \" data-mask-text=\"false\" src=\"https://cdn.vox-cdn.com/thumbor/oE1f7IJ14APQQX4y2b7SZ4Vrthg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/23212711/Fig_04_Final.jpg\">      <cite>Image: DeepMind</cite>      <figcaption><em>An example interface of AlphaCode tackling a coding challenge. The input is given as it is to humans on the left and the output generated on the right. </em></figcaption>  </figure><p id=\"2tClGT\">DeepMind notes that AlphaCode’s current skill set is only currently applicable within the domain of competitive programming but that its abilities open the door to creating future tools that make programming more accessible and one day fully automated. </p><p id=\"a1ENly\">Many other companies are working on similar applications. For example, Microsoft and the AI lab OpenAI have adapted the latter’s language-generating program GPT-3 to function <a href=\"https://www.theverge.com/2021/5/25/22451144/microsoft-gpt-3-openai-coding-autocomplete-powerapps-power-fx\">as an autocomplete program</a> that finishes strings of code. (Like GPT-3, AlphaCode is also based on an AI architecture known as a transformer, which is particularly adept at parsing sequential text, both natural language and code). For the end user, these systems work just like Gmails’ Smart Compose feature — suggesting ways to finish whatever you’re writing.</p><div class=\"c-float-right\"><aside id=\"KzJeOE\"><q>AI systems have been criticized for producing buggy, vulnerable code</q></aside></div><p id=\"9DxIJa\">A lot of progress has been made developing AI coding systems in recent years, but these systems are far from ready to just take over the work of human programmers. The code they produce is often buggy, and because the systems are usually trained on libraries of public code, they sometimes reproduce material that is copyrighted. </p><p id=\"0zCL24\">In <a href=\"https://arxiv.org/abs/2108.09293\">one study</a> of an AI programming tool named Copilot developed by code repository GitHub, researchers found that around 40 percent of its output contained security vulnerabilities. Security analysts <a href=\"https://www.wired.com/story/ai-write-code-like-humans-bugs/\">have even suggested</a> that bad actors could intentionally write and share code with hidden backdoors online, which then might be used to train AI programs that would insert these errors into future programs. </p><p id=\"13pdyT\">Challenges like these mean that AI coding systems will likely be integrated slowly into the work of programmers — starting as assistants whose suggestions are treated with suspicion before they are trusted to carry out work on their own. In other words: they have an apprenticeship to carry out. But so far, these programs are learning fast. </p>",
  "image": "https://cdn.vox-cdn.com/thumbor/2_fYPSyvvHSDJ0NIvrahsyHWiO8=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/13292779/acastro_181017_1777_brain_ai_0002.jpg",
  "description": "DeepMind teaches AI to code.",
  "publisher": "The Verge",
  "publisherUrl": "https://www.theverge.com/"
}